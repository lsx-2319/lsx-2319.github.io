[{"content":"在网络故障排查中，conntrack 是洞察内核连接跟踪状态的核心工具。它不直接配置网络，而是像 “网络连接监控器”，通过显示连接状态帮助你判断报文在何处被处理或丢弃。\n📊 Conntrack 核心价值：连接状态视角 传统工具（如 ping, tcpdump）告诉你报文“到哪里”，而 conntrack 告诉你内核认为某个连接“处于什么状态”。这对于诊断防火墙、NAT和状态化过滤引起的问题至关重要。\n🛠️ 常用命令与实战解读 以下命令需 root 权限（sudo）。\n命令 用途 关键输出解读与示例 conntrack -L 列出当前所有连接跟踪条目。 查看所有被跟踪的连接（TCP/UDP/ICMP）。 conntrack -L -o extended （显示更详细字段） conntrack -E 实时监控新连接事件。 故障排查首选。实时观察新连接是否被创建。 conntrack -E -p tcp --dport 80 （监控特定流量） conntrack -L -p icmp 查看特定协议（如ICMP）的连接。 检查 ping 产生的跟踪条目。**状态应为 **ESTABLISHED。 conntrack -L -p udp 查看UDP“连接”（如DNS）。 UDP虽无状态，但内核仍会跟踪会话。用于检查DNS查询。 conntrack -L -s \u0026lt;IP\u0026gt; 过滤查看指定源IP的连接。 定位来自特定主机的所有连接状态。 conntrack -D 手动删除连接跟踪条目。 强制清除某条过期或僵死的连接，常用于测试。 conntrack -D -p tcp --orig-port-dst 80 🔍 经典故障排查场景与命令流 当网络不通时，可按此流程使用 conntrack 进行状态分析：\n场景一：排查 ping (ICMP) 不通 在一个终端启动监控： 1 conntrack -E -p icmp 在另一个终端执行 ping \u0026lt;目标IP\u0026gt;。 观察监控终端： 无任何输出：报文未到达 conntrack 模块，问题可能在路由或出站防火墙之前。 出现 [NEW] 事件：请求已发出，被内核跟踪。 随后出现 [UPDATE] 事件：收到回复，连接建立成功。若只有 [NEW] 无 [UPDATE]，则回复未被收到，问题可能在对方主机、中间网络或本机入站规则。 场景二：排查 TCP/UDP 服务不通（如SSH、Web） 监控相关连接： 1 conntrack -E -p tcp --dport 22 # 监控SSH 尝试建立连接（如 ssh 命令）。 观察状态： 理想情况：看到 [NEW] -\u0026gt; [UPDATE]（状态变为 ESTABLISHED）。 只有 [NEW]：你的请求出去了，但对方回复没回来。检查对方服务、中间网络及本机 INPUT 链/安全组入方向。 无事件：报文在到达 conntrack 前被丢弃。检查本机 OUTPUT** 链/安全组出方向**、路由。 场景三：诊断 conntrack 表满导致的丢包 这是经典故障，症状为连接随机超时或中断。\n检查日志： 1 2 dmesg | grep \u0026#34;conntrack\u0026#34; # 关键信息：`nf_conntrack: table full, dropping packet` 查看当前计数与上限： 1 2 3 sysctl net.netfilter.nf_conntrack_count sysctl net.netfilter.nf_conntrack_max # 如果 count 接近 max，则表将满 临时解决方案： 1 2 3 4 # 增大表上限 sysctl -w net.netfilter.nf_conntrack_max=524288 # 缩短某些超时时间，加速条目回收（谨慎调整） sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=1200 💎 核心要点总结 conntrack -E** 是你的“第一双眼睛”**：在测试前打开，直接观察连接是否被内核成功跟踪。 状态是关键：NEW 表示请求已发出；ESTABLISHED 表示双向通信已得到内核放行；UNREPLIED 通常意味着只有单边流量。 conntrack** 与 tcpdump 是黄金组合**：conntrack 看状态，tcpdump 抓报文，两者结合可以精确定位报文是在哪一层被丢弃。 表满是常见病：在网关、高连接数服务器上，务必监控 nf_conntrack_count，并适当调整 max 值。 下次遇到“时通时不通”或“特定协议不通”的诡异问题时，首先运行 conntrack -E 进行实时监控，它能为你提供最直接的线索。\n","date":"2025-12-30T00:00:00Z","permalink":"https://example.com/p/%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C%E5%B7%A5%E5%85%B7conntrack/","title":"网络排障工具conntrack"},{"content":"深入解析Docker容器的rootfs：容器独立性的基石 你是否曾经好奇，为什么Docker容器能在毫秒内启动，并且每个容器都拥有自己独立的文件系统，互不干扰？今天我们就来揭开这个秘密——这一切的核心在于rootfs（根文件系统）。\n什么是rootfs？ 简单来说，rootfs是容器的\u0026quot;根目录\u0026quot;。当你运行一个容器时，Docker会为这个容器创建一个独立的、隔离的文件系统视图，这就是rootfs。它让容器内的进程认为自己拥有完整的Linux文件系统，从/根目录开始。\n但有趣的是，这个\u0026quot;完整\u0026quot;的文件系统实际上是由多个只读层和一个可写层组合而成的。这正是Docker设计的精妙之处。\n为什么需要rootfs？ 想象一下传统虚拟机：每个虚拟机都需要完整的操作系统副本，包括内核、系统库、配置文件等。这带来了巨大的资源开销和启动延迟。\n相比之下，容器通过rootfs实现了：\n轻量级：多个容器可以共享基础镜像层 快速启动：无需启动完整操作系统 隔离性：每个容器有独立的文件系统视图 可重复性：相同的镜像总是产生相同的文件系统 rootfs的分层架构 让我们通过一个实际的Dockerfile来理解：\n1 2 3 4 FROM ubuntu:20.04 # 第一层：基础镜像 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y python3 # 第二层：安装Python COPY app.py /app/ # 第三层：添加应用代码 CMD [\u0026#34;python3\u0026#34;, \u0026#34;/app/app.py\u0026#34;] 构建这个镜像时，Docker会创建三个只读层，加上容器运行时的一个可写层：\n1 2 3 4 5 6 7 容器层（可写层） ↑ 第三层（只读）：app.py ↑ 第二层（只读）：Python3安装 ↑ 第一层（只读）：Ubuntu基础系统 联合文件系统（UnionFS） 这是实现分层架构的技术核心。UnionFS允许将多个目录（层）合并成一个统一的视图。常见的实现有：\nOverlayFS（现代Linux默认） AUFS（早期Docker常用） Device Mapper Btrfs 以OverlayFS为例，它包含：\nlowerdir：只读的镜像层 upperdir：可写的容器层 merged：合并后的统一视图 动手实验：查看容器的rootfs 让我们实际看看rootfs的结构：\n1 2 3 4 5 6 7 8 # 运行一个简单容器 docker run -d --name test-nginx nginx:alpine # 查找容器的存储位置 docker inspect test-nginx | grep \u0026#34;GraphDriver\u0026#34; # 查看OverlayFS结构（路径可能因系统而异） ls /var/lib/docker/overlay2/\u0026lt;container-id\u0026gt;/ 你会看到类似这样的结构：\ndiff/：可写层的变化 merged/：合并后的完整视图 work/：OverlayFS内部工作目录 lower：指向只读层的链接 rootfs的工作原理 写时复制（Copy-on-Write） 这是Docker高效存储的关键。当容器需要修改文件时：\n读取文件：直接从底层只读层读取 修改文件：复制到可写层，然后在可写层修改 读取修改后的文件：优先从可写层读取 这种机制意味着：\n相同的基础镜像可以被多个容器共享 只有实际修改的文件才会占用额外空间 容器删除时，只需删除可写层 容器内的文件系统视角 在容器内部，所有层被合并成一个统一的/根目录：\n1 2 3 4 5 6 7 8 # 进入容器查看 docker exec -it test-nginx sh # 查看根目录，这是所有层的合并视图 ls / # 退出容器 exit rootfs的实际意义 1. 高效的镜像分发 Docker Hub上的镜像以层为单位存储和传输。当你拉取镜像时：\n已存在的层不会重复下载 只有新层需要传输 层可以并行下载 2. 快速构建 Docker构建利用层缓存：\n1 2 3 4 5 6 # 如果基础层未变，这些指令会使用缓存 RUN apt-get update RUN apt-get install -y curl # 这之后的指令才会重新执行 COPY app.py /app/ 3. 空间优化 多个容器共享基础层，极大减少存储占用。\n4. 安全性 通过只读的基础层，减少了攻击面。可以使用docker run --read-only运行只读容器。\n常见误区澄清 误区1：每个容器都有完整的操作系统副本 事实：容器共享主机内核，只包含必要的用户空间文件。\n误区2：容器内的修改会影响镜像 事实：容器的修改只存在于可写层，不会影响底层的只读镜像层。\n误区3：删除容器会释放所有空间 事实：只有可写层被删除，共享的镜像层仍然存在。\n最佳实践 最小化镜像层数 合并相关指令 使用多阶段构建 合理安排指令顺序 1 2 3 4 # 将变化频繁的层放在后面 COPY package.json /app/ RUN npm install # 这层变化较少 COPY . /app/ # 这层变化频繁 使用.dockerignore 避免不必要的文件进入镜像 减少构建上下文大小 定期清理 1 2 # 清理未使用的镜像和层 docker system prune -a 总结 rootfs是Docker容器技术的核心之一，它通过分层架构和联合文件系统，实现了容器的轻量、快速和隔离。理解rootfs不仅有助于我们更好地使用Docker，还能帮助我们设计更高效的容器化应用。\n","date":"2025-12-07T00:00:00Z","permalink":"https://example.com/p/%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90docker%E5%AE%B9%E5%99%A8%E7%9A%84rootfs/","title":"深入解析Docker容器的rootfs"},{"content":"DPDK技术原理与架构 技术原理与架构 由于采用软件转发和软件交换技术，单服务器内部的转发能力是 NFV 系统的主要性能瓶颈。在各类高速转发的 NFV 应用中，数据报文从网卡中接收，再传送到虚拟化的用户态应用程序（VNF）处理，整个过程要经历 CPU 中断处理、虚拟化 I/O 与地址映射转换、虚拟交换层、网络协议栈、内核上下文切换、内存拷贝等多个费时的 CPU 操作和 I/O 处理环节。\n业内通常采用消除海量中断、旁路内核协议栈、减少内存拷贝、CPU 多核任务分担、Intel VT 等技术来综合提升服务器数据平面的报文处理性能，普通用户较难掌握。业界迫切需要一种综合的性能优化方案，同时提供良好的用户开发和商业集成环境，DPDK 加速技术方案成为其中的典型代表。\nDPDK 是一个开源的数据平面开发工具集，提供了一个用户空间下的高效数据包处理库\n函数，它通过环境抽象层旁路内核协议栈、轮询模式的报文无中断收发、优化内存/缓冲区/队列管理、基于网卡多队列和流识别的负载均衡等多项技术，实现了在 x86 处理器架构下的高性能报文转发能力，用户可以在 Linux 用户态空间开发各类高速转发应用，也适合与各类商业化的数据平面加速解决方案进行集成。\n英特尔在 2010 年启动了对 DPDK 技术的开源化进程，于当年 9 月通过 BSD 开源许可协议正式发布源代码软件包，并于 2014 年 4 月在 www.dpdk.org 上正式成立了独立的开源社区平台，为开发者提供支持。开源社区的参与者们大幅推进了 DPDK 的技术创新和快速演进，而今它已发展成为 SDN 和 NFV 的一项关键技术。\n软件架构 DPDK 的组成架构如图所示，相关技术原理概述如下：\n在最底部的内核态（Linux Kernel）DPDK 有两个模块：KNI 与 IGB_UIO。其中，KNI 提供给用户一个使用 Linux 内核态的协议栈，以及传统的 Linux 网络工具（如ethtool, ifconfig）。IGB_UIO（igb_uio.ko 和 kni.ko. IGB_UIO）则借助了 UIO 技术，在初始化过程中将网卡硬件寄存器映射到用户态。\n如图，DPDK 的上层用户态由很多库组成，主要包括核心部件库（Core Libraries）、平台相关模块(Platform)、网卡轮询模式驱动模块（PMD-Natives\u0026amp;Virtual）、QoS 库、报文转发分类算法（Classify）等几大类，用户应用程序可以使用这些库进行二次开发，下面分别简要介绍。\n核心部件库：该模块构成的运行环境是建立在 Linux 上，通过环境抽象层(EAL)的运行环境进行初始化，包括：HugePage 内存分配、内存/缓冲区/队列分配与无锁操作、CPU 亲和性绑定等；其次，EAL 实现了对操作系统内核与底层网卡 I/O 操作的屏蔽（I/O 旁路了内核及其协议栈），为 DPDK 应用程序提供了一组调用接口，通过 UIO 或 VFIO 技术将 PCI 设备地址映射到用户空间，方便了应用程序调用，避免了网络协议栈和内核切换造成的处理延迟。\n另外，核心部件还包括创建适合报文处理的内存池、缓冲区分配管理、内存拷贝、以及定时器、环形缓冲区管理等。\n平台相关模块：其内部模块主要包括 KNI、能耗管理以及 IVSHMEM 接口。其中，KNI 模块主要通过 kni.ko 模块将数据报文从用户态传递给内核态协议栈处理，以便用户进程使用传统的 socket 接口对相关报文进行处理；能耗管理则提供了一些 API，应用程序可以根据收包速率动态调整处理器频率或进入处理器的不同休眠状态；另外，IVSHMEM 模块提供了虚拟机与虚拟机之间，或者虚拟机与主机之间的零拷贝共享内存机制，当 DPDK 程序运行时，IVSHMEM 模块会调用核心部件库 API，把几个 HugePage 映射为一个 IVSHMEM 设备池，并通过参数传递给 QEMU，这样，就实现了虚拟机之间的零拷贝内存共享。\n轮询模式驱动模块：PMD 相关 API 实现了在轮询方式下进行网卡报文收发，避免了常规报文处理方法中因采用中断方式造成的响应延迟，极大提升了网卡收发性能。此外，该模块还同时支持物理和虚拟化两种网络接口，从仅仅支持 Intel 网卡，发展到支持 Cisco、Broadcom、Mellanox、Chelsio 等整个行业生态系统,以及基于 KVM、VMWARE、 XEN 等虚拟化网络接口的支持。\nDPDK 还定义了大量 API 来抽象数据平面的转发应用，如 ACL、QoS、流分类和负载均衡等。并且，除以太网接口外，DPDK 还在定义用于加解密的软硬件加速接口（Extensions）。\n大页技术 处理器的内存管理包含两个概念：物理内存和虚拟内存。Linux 操作系统里面整个物理\n内存按帧（frames）来进行管理，虚拟内存按照页（page）来进行管理。内存管理单元（MMU）完成从虚拟内存地址到物理内存地址的转换。内存管理单元进行地址转换需要的信息保存在一个叫页表（page table）的数据结构里面，页表查找是一种极其耗时的操作。\nx86 处理器硬件在缺省配置下，页的大小是 4K，但也可以支持更大的页表尺寸，例如\n2M 或 1G 的页表。使用了大页表功能后，一个 TLB 表项可以指向更大的内存区域，这样可以大幅减少 TLB miss 的发生。早期的 Linux 并没有利用 x86 硬件提供的大页表功能，仅在 Linux内核 2.6.33 以后的版本，应用软件才可以使用大页表功能，具体的介绍可以参见 Linux 的大页表文件系统（hugetlbfs）特性。\nDPDK 则利用大页技术，所有的内存都是从 HugePage 里分配，实现对内存池(mempool)的管理，并预先分配好同样大小的 mbuf，供每一个数据包使用。\n轮询技术 为了减少中断处理开销，DPDK 使用了轮询技术来处理网络报文。网卡收到报文后，直\n接将报文保存到处理器 cache 中（有 DDIO（Direct Data I/O）技术的情况下），或者保存到内存中（没有 DDIO 技术的情况下），并设置报文到达的标志位。应用软件则周期性地轮询报文到达的标志位，检测是否有新报文需要处理。整个过程中完全没有中断处理过程，因此应用程序的网络报文处理能力得以极大提升。\nCPU 亲和技术 现代操作系统都是基于分时调用方式来实现任务调度，多个进程或线程在多核处理器的\n某一个核上不断地交替执行。每次切换过程，都需要将处理器的状态寄存器保存在堆栈中，并恢复当前进程的状态信息，这对系统其实是一种处理开销。将一个线程固定一个核上运行，可以消除切换带来的额外开销。另外将进程或者线程迁移到多核处理器的其它核上进行运行时，处理器缓存中的数据也需要进行清除，导致处理器缓存的利用效果降低。\nCPU 亲和技术，就是将某个进程或者线程绑定到特定的一个或者多个核上执行，而不被\n迁移到其它核上运行，这样就保证了专用程序的性能。\nDPDK 使用了 Linux pthread 库，在系统中把相应的线程和 CPU 进行亲和性绑定, 然后\n相应的线程尽可能使用独立的资源进行相关的数据处理。\n","date":"2025-09-18T00:00:00Z","permalink":"https://example.com/p/dpdk%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E4%B8%8E%E6%9E%B6%E6%9E%84/","title":"dpdk技术原理与架构"},{"content":"最近工作中遇到一个问题，在一个vpc下面有两台虚拟机，其中一台机器绑定了eip可以访问互联网，另一台没有绑，没有公网的机器想通过绑定eip的机器来访问公网，我采用了frp中的sock5代理来实现这个需求，首先分别介绍一下frp和socks5代理协议。\n1、frp简介 frp 是一个可用于内网穿透的高性能的反向代理应用，支持TCP、UDP协议，为HTTP和HTTPS应用协议提供了额外的能力，且尝试性支持了点对点穿透。frp 采用go语言开发。更多的人使用 frp 是为了进行反向代理，满足通过公网服务器访问处于内网的服务，如访问内网web服务，远程ssh内网服务器，远程控制内网NAS等，实现类似花生壳、ngrok等功能。而对于内网渗透来讲，这种功能恰好能够满足我们进行内网渗透的流量转发。FRP最大的一个特点是使用SOCKS代理，而SOCKS是加密通信的，类似于做了一个加密的隧道，可以把外网的流量，通过加密隧道穿透到内网。效果有些类似于VPN。\n2、SOCKS介绍 SOCKS全称是SOCKet Secure，是一种网络传输协议，主要用于客户端与外网服务器之间通讯的中间传递。在OSI模型中，SOCKS是会话层的协议，位于表示层与传输层之间，最新协议是SOCKS5。\nSOCKS5原理\n①首先客户端向代理服务器发出请求信息，用以协商版本和认证方法。随后代理服务器应答，将选择的方法发送给客户端。\n②客户端和代理服务器进入由选定认证方法所决定的子协商过程，子协商过程结束后，客户端发送请求信息，其中包含目标服务器的IP地址和端口。代理服务器验证客户端身份，通过后会与目标服务器连接，目标服务器经过代理服务器向客户端返回状态响应。\n③连接完成后，代理服务器开始作为中转站中转数据。\n3、操作步骤 1、在vm2 上下载frp\n访问网址https://github.com/fatedier/frp/releases\n根据自己的操作系统下载对应的版本\n2、然后进行解压，解压之后下面有几个文件\n3、编辑frpc.toml和frps.toml。\nfrpc.toml\nfrps.toml\n4、然后分别启动frps和frpc\n./frps -c frps.toml\n./frpc -c frpc.toml\n5、然后vm1便可以通过访问vm2ip:1080来访问到公网\n","date":"2025-08-24T00:00:00Z","permalink":"https://example.com/p/frp-socks5%E4%BB%A3%E7%90%86%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF/","title":"frp socks5代理应用场景"},{"content":"IGMPv2 vs IGMPv3：你的组播网络该升级了吗？ 场景： 办公室里，小王正对着卡成PPT的直播培训抓狂，隔壁组的4K监控大屏却流畅无比。网管老张神秘一笑：“想知道秘密？关键在于组播协议的选择！”\n没错，当海量设备需要同时接收同一份数据（比如视频流、在线会议），单播（点对点）广播（无差别轰炸）都会拖垮网络。组播（Multicast） 才是高效解决方案，而 IGMP（Internet Group Management Protocol） 就是设备与路由器间沟通“谁想看什么频道”的核心语言。今天我们就来拆解最常用的两个版本：IGMPv2 和 IGMPv3。\n📌 核心任务：举手报名“我想看这个频道！” 无论是v2还是v3，核心任务一致：\n设备（成员） 告诉本地路由器：“我要加入XX组播组（频道）！” 路由器 负责将组播流转发给所有“报名”的设备； 设备不想看了，通知路由器：“我退出了！” 区别在于“报名”和“退出”的精细程度和效率！\nIGMPv2：基础版“大锅饭”频道订阅 想象一个老式收音机📻：\n加入频道： 你只能调到某个频率（组播组地址，如239.1.2.3）。只要调到这个频率，就能收到所有在这个频率上播放的节目（源），无法选择具体电台。 离开频道： 直接关掉收音机（发送离开报文）。路由器会快速确认是否还有人听这个频道。 IGMPv2 关键特点：\nASM (Any-Source Multicast) 模型： 只关心组地址。加入239.1.2.3，意味着接收任何源发送到这个地址的流量。 离开机制 (Leave Message)： 成员离开时主动发送离开报文，路由器触发特定组查询（Group-Specific Query）快速确认是否还有成员。显著减少了离开延迟！ (相比v1靠超时判断)。 查询器选举 (Querier Election)： 同一网段多个路由器时，通过比较IP地址自动选举一个负责发送查询报文（谁是老大谁来问）。 成员报告抑制： 多个成员想加入同一组时，后听到报告的成员会抑制自己的报告，减少网络流量。 ❌** IGMPv2 痛点：**\n无法选择节目源！ 加入239.1.2.3，会收到所有发送到此地址的流量，即使是恶意流量或低质量源，造成带宽浪费和安全风险（源泛滥）。 🚀 IGMPv3：升级版“精准点播”源过滤 想象一个智能电视盒子📺：\n加入频道： 你可以选择239.1.2.3这个“频道”，并且精确指定只接收来自“央视”(源S1)的节目，或者排除“某个地方台”(源S2)的节目。 离开/修改： 可以随时修改你的“频道订阅清单”（源列表）。 IGMPv3 核心飞跃：SSM (Source-Specific Multicast) 模型支持！\n源过滤 (Source Filtering)： 成员报告报文中包含组地址 + 源地址列表。 INCLUDE 模式： 只接收列表内指定源发送到该组的流量。(G, INCLUDE, {S1, S2}) -\u0026gt; 只看S1和S2发的G组内容。 EXCLUDE 模式： 接收列表外所有源发送到该组的流量。(G, EXCLUDE, {S3}) -\u0026gt; 接收除了S3以外任何源发的G组内容。 更灵活的状态维护： 主机为每个组维护一个“源过滤”状态，可随时发送报告更新（加入新源、移除旧源、改变模式）。 兼容性： 通常兼容IGMPv2成员（路由器工作在v3时，会以ASM方式处理v2报告）。 报告聚合： 更复杂的报告机制，但设计上仍考虑减少报文数量。 📊 IGMPv2 与 IGMPv3 核心差异速查表 特性 IGMPv2 IGMPv3 优势对比 模型 ASM (Any-Source Multicast) ASM + SSM (Source-Specific) v3支持精确指定/排除源！ 加入粒度 仅组地址 (G) 组地址 + 源地址列表 (G, S) v3精细控制源，避免无效流量 过滤模式 无 (隐含接收所有源) INCLUDE / EXCLUDE v3提供两种灵活的订阅方式 离开机制 有 (发送离开报文) 有 (通过发送更新模式的报告实现) 两者离开效率都远高于v1 查询器选举 有 (基于IP地址) 有 (机制相同) 相同 主要优势 实现简单，广泛支持 安全性高，带宽利用率高，支持SSM应用 v3是现代组播应用的基石 典型应用 早期视频广播、基础组播应用 IPTV (频道切换)、视频会议源选择、安全组播、金融行情 v3满足高性能、高安全需求 🧠 为什么你需要关心IGMPv3？ 省带宽、提效率： 只接收需要的源，避免垃圾组播流浪费宝贵带宽，尤其在视频等高流量场景。 增强安全性： 天然抵御来自未授权源的组播洪泛攻击。 支撑现代应用： IPTV换台快、大型视频会议选择发言人流畅、金融行情精确分发\u0026hellip;都依赖SSM，而SSM的核心就是IGMPv3！ 未来趋势： 随着万兆网络普及和组播应用深化，IGMPv3已成为主流和推荐配置。 💡 总结：升级你的组播“沟通语言” IGMPv2 是基础： 解决了“我要看G频道”和“我不看了”的问题，效率比v1高，但无法选择节目源，如同吃大锅饭。 IGMPv3 是飞跃： 在v2基础上，增加了强大的源过滤能力（INCLUDE/EXCLUDE），实现了“我要看G频道，但只要/不要S1/S2\u0026hellip;提供的节目”的精准控制。这是高效、安全、现代组播应用的基石。 📣** 行动建议：** 如果你的网络中存在视频分发、实时通信、大规模数据推送等组播应用，尤其是对带宽和安全性有要求，强烈建议部署IGMPv3！检查你的路由器、交换机和终端操作系统是否支持并启用了它。别让过时的协议，成为网络流畅体验的瓶颈！\n","date":"2025-08-17T00:00:00Z","permalink":"https://example.com/p/%E7%BB%84%E6%92%ADigmpv2%E5%92%8Cigmpv3%E5%8C%BA%E5%88%AB/","title":"组播igmpv2和igmpv3区别"},{"content":"组播 IGMP 协议详解\n在日常的网络传输中，我们最常听到的就是 单播（Unicast） 和 广播（Broadcast）。单播就像你给朋友单独发一条微信，只发给他一个人；广播则像在微信群里吼一嗓子，所有人都能看到。\n那 组播（Multicast） 呢？它更像是建立一个兴趣小组，只有加入这个小组的人才能收到你发的内容。而 IGMP（Internet Group Management Protocol）就是负责“管理小组成员”的协议。\n一、IGMP 是什么？ IGMP（Internet Group Management Protocol） 是 IPv4 网络中用于管理组播组成员关系的协议。\n简单来说，它帮助网络设备（尤其是路由器）知道：\n哪些主机想加入某个组播组 哪些主机不再需要组播数据 当前有哪些活跃的组播成员 它工作在 网络层，但运行在主机和相邻路由器之间，并不会跨越多个路由器去传播。\n二、组播的实际例子 想象一下，你所在的公司要直播一场全员大会。\n如果用单播，每个员工的电脑都要跟服务器建立一条独立的连接，这会让服务器压力山大。 如果用广播，虽然省事，但不仅公司员工，连打印机、考勤机这些设备也会被迫收到直播数据（没用还浪费带宽）。 用组播就完美了：只有主动“加入直播组”的员工电脑才能收到直播视频数据，其他设备完全不受影响。 这个“加入直播组”的过程，就是通过 IGMP 协议 来实现的。\n三、IGMP 的工作机制 IGMP 的运作过程，核心就是三种报文：\nMembership Query（成员查询）\n由路由器发出，询问“大家谁还在某个组播组里？” Membership Report（成员报告）\n由主机发出，表示“我还在某个组里，请继续给我发数据。” Leave Group（离开组）\n由主机发出，表示“我不看这个组播数据了，可以停了。” 四、IGMP 的版本演进 IGMP 目前有三个版本，每一代都比上一代更智能。\nIGMPv1（最基础） 主机可以加入组播组，但没有“离开组”的机制，需要路由器超时后才会停止发送数据。 IGMPv2（更高效） 增加了 Leave Group 报文，可以主动告诉路由器自己不再需要数据。 支持指定组的查询，提高了管理效率。 IGMPv3（更灵活） 支持 源特定组播（SSM），即只接收来自特定源 IP 的组播数据，安全性和精准度更高。 五、用例场景再深入一点 我们回到公司的直播例子。\n假设公司 IT 部署了组播路由器，并分配了一个组播地址 239.1.1.1 作为“全员大会直播”频道：\n当你打开直播软件时，软件会通过 IGMP Report 告诉路由器： “我要加入 239.1.1.1 组，请把数据发给我。”\n路由器记录下来，并把直播视频数据转发到你的电脑。 当直播结束，你关闭软件，软件会发一个 Leave Group 给路由器： “我退出 239.1.1.1 组了，可以停发给我。”\n如果有很多人退出，路由器可能会发 Membership Query 确认是否还有人需要这个直播数据，如果没人了，就停止该组播数据的转发。 这样一来，服务器只需要发一份直播流到路由器，路由器再复制给组内成员，比单播节省了大量带宽，也避免广播的无差别轰炸。\n六、IGMP 与其他协议的关系 IGMP 只是组播管理的**“前台登记员”**，真正负责组播转发的，是像 PIM（Protocol Independent Multicast） 这样的路由协议。\n可以这么理解：\nIGMP：负责“谁在看这个频道” PIM：负责“把频道信号送到这些人手里” 两者配合，才能让组播数据准确高效地送达。\n七、总结 IGMP 协议虽然听起来是个小角色，但在组播体系中它就像一个观众登记系统：\n它让路由器知道谁需要组播数据，谁已经离开 它帮助节省带宽、减少无效数据传输 它的版本升级让组播管理更加高效和安全 下一次你在公司看直播、在局域网玩联机游戏、或者家里 IPTV 看电视时，很可能后台就有 IGMP 在悄悄帮你管理数据流。\n所以，虽然它的存在感不高，但没有它，组播的世界就会变得一团糟。\n","date":"2025-08-09T00:00:00Z","permalink":"https://example.com/p/%E7%BB%84%E6%92%ADigmp%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/","title":"组播igmp协议详解"},{"content":"好的，我们来详细解析一下 OSPF（Open Shortest Path First） 协议。它是网络协议栈中非常重要的内部网关协议（IGP），用于在单一自治系统（AS） 内部的路由器之间动态地交换路由信息并计算最优路径。\n核心定位与目标 类型： 链路状态路由协议（Link-State Routing Protocol）。这是理解 OSPF 的关键。 目标： 让 AS 内部的所有路由器都拥有一张完全相同的、描述整个 AS 网络拓扑结构的“地图”（链路状态数据库 - LSDB），并基于此地图独立地使用 SPF（Shortest Path First，即 Dijkstra）算法 计算出到达 AS 内所有网络的最短（最优）路径树，最终形成路由表。 对比 RIP： 相比距离矢量协议（如 RIP），OSPF 收敛速度快得多（网络变化时路由信息快速稳定），支持大型网络（通过区域划分），无路由环路（由 SPF 算法保证），支持 VLSM 和 CIDR，提供更丰富的路由度量（Cost）。 关键特性和概念 链路状态（Link-State）：\n每个路由器不再像 RIP 那样只告诉邻居“我到某个网络的距离是多少”，而是主动描述自己直连的网络状态。 这个描述信息称为 LSA（Link State Advertisement - 链路状态通告）。它包含的信息如：该路由器连接了哪些网络、邻居路由器是谁、连接到这些链路/邻居的“代价（Cost）”是多少。 路由器将生成的 LSA 泛洪（Flooding） 给 OSPF 域内的所有其他路由器（在区域限制内）。 链路状态数据库（LSDB - Link State Database）：\n每个路由器收到域内所有其他路由器发来的 LSA 后，会把这些 LSA 存储在自己的 LSDB 中。 理想情况下，同一个区域（Area）内所有路由器的 LSDB 内容是完全一致的。这是路由器能独立、一致地计算最短路径树的基础。 SPF（Dijkstra）算法：\n每个路由器都独立地以自己的视角（作为根）运行 SPF 算法。 算法基于 LSDB 中存储的完整拓扑信息（节点=路由器/网络，边=链路，边权=Cost），计算出一棵到达所有目标网络的最短路径树（SPT - Shortest Path Tree）。 这棵树上的路径就是该路由器认为的到达各网络的最优无环路径。 计算完成后，最优路径被安装到路由表中。 区域（Area）划分：\n为了解决大型网络 LSDB 过大、SPF 计算负担过重、LSA 泛洪范围过广的问题，OSPF 引入了层次化结构 - 区域。 骨干区域（Area 0）： OSPF 域的核心。所有其他区域（非骨干区域）必须直接连接到骨干区域。骨干区域负责在非骨干区域之间传递路由信息。区域 0 的 LSDB 包含所有区域的汇总路由信息。 非骨干区域（Area 1, Area 2, etc）： 内部路由器的 LSDB 只包含本区域的详细拓扑信息和来自其他区域的汇总路由信息（由 ABR 生成）。这大大减小了 LSDB 的规模，限制了 LSA 泛洪的范围，提高了网络的扩展性和稳定性。 区域边界路由器（ABR - Area Border Router）： 连接多个区域（至少一个区域是 Area 0）的路由器。它负责： 将其连接区域的内部路由信息汇总后通告到骨干区域。 将骨干区域学到的其他区域的汇总路由信息（以及外部路由）通告到其连接的非骨干区域。 维护所连接每个区域的独立 LSDB。 邻居和邻接关系建立：\n邻居（Neighbor）： 通过 Hello 协议发现的、连接在同一链路上的其他 OSPF 路由器。两台路由器交换 Hello 包，确认参数（如 Area ID, Hello/Dead Interval, 认证密码等）匹配后成为邻居。 邻接（Adjacency）： 在邻居关系基础上，需要同步 LSDB 的路由器对之间建立邻接关系。只有建立了邻接关系的路由器才会交换 LSA。 建立过程关键状态： Down -\u0026gt; Init： 收到邻居的 Hello 包（包中没有包含自己的 Router ID）。 Init -\u0026gt; 2-Way： 收到邻居的 Hello 包（包中已包含自己的 Router ID）。选举 DR/BDR（如果需要）。 2-Way -\u0026gt; ExStart： （仅与 DR/BDR 或点对点链路上的邻居）协商主从关系和初始 DD 序列号。 ExStart -\u0026gt; Exchange： 交换 DBD 包（Database Description），描述自己 LSDB 的摘要（LSA 头部）。 Exchange -\u0026gt; Loading： 通过 LSR（Link State Request）请求和 LSU（Link State Update）发送自己缺少的或更新的完整 LSA。 Loading -\u0026gt; Full： LSDB 同步完成，邻接关系建立成功。开始定期发送 Hello 包维护邻居关系，并泛洪 LSA 更新。 OSPF 工作流程概览 启动与邻居发现： 路由器在接口启用 OSPF，开始发送 Hello 包。发现邻居。 邻接关系建立与 LSDB 同步： 与符合条件的邻居（如 DR/BDR 或点对点邻居）建立邻接，交换 DBD、LSR、LSU 包，同步 LSDB。 维护 LSDB： 当本地链路状态发生变化（如接口 Up/Down，Cost 改变），路由器产生新的 LSA。 新的 LSA 被泛洪给所有建立了邻接关系的邻居。 邻居收到新 LSA 后，更新自己的 LSDB，并继续泛洪（除始发路由器外），同时立即触发一次 SPF 计算（部分计算或完全计算，取决于 LSA 类型）。 定期发送 LSA（老化时间默认为 30 分钟），收到后会刷新老化计时器。 路由计算： 路由器使用 SPF 算法基于最新的 LSDB 计算最短路径树，将最优路径安装到路由表中。 维护邻居关系： 通过周期性的 Hello 包（默认 10 秒/30 秒 Dead）维护邻居关系。若在 Dead Interval 内未收到邻居的 Hello 包，则认为邻居失效，清除邻接关系，移除该邻居产生的 LSA，触发新的 SPF 计算。 总结 OSPF 是一个强大、复杂且高度可扩展的链路状态内部网关协议。其核心在于每个路由器都拥有相同的网络拓扑地图（LSDB），并独立运行 SPF 算法计算无环的最短路径。通过区域划分解决了大型网络的可扩展性问题。理解 LSA 的类型和作用、邻居/邻接建立过程、DR/BDR 角色、区域概念（尤其是 Area 0）以及 SPF 计算原理是掌握 OSPF 的关键。虽然配置和设计比 RIP 复杂，但其快速收敛、无环、可扩展的特性使其成为现代企业网和数据中心内部网络路由的首选协议之一。\n","date":"2025-07-29T00:00:00Z","permalink":"https://example.com/p/ospf%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/","title":"OSPF协议详解"},{"content":"RoCE 什么是RoCE？ 全称RDMA over Converged Ethernet，RoCE是一种基于以太网的远程直接内存访问（RDMA）技术，旨在通过优化协议栈和硬件卸载，实现高性能、低延迟的数据传输。其核心是通过绕过操作系统内核，减少数据复制和CPU开销，尤其适用于AI训练、超算和分布式存储等场景。\n从 2010 年开始，RMDA 开始引起越来越多的关注，当时IBTA发布了第一个在融合以太网 (RoCE) 上运行 RMDA 的规范。然而，最初的规范将 RoCE 部署限制在单个第 2 层域，因为 RoCE 封装帧没有路由功能。2014 年，IBTA 发布了 RoCEv2，它更新了最初的 RoCE 规范以支持跨第 3 层网络的路由，使其更适合超大规模数据中心网络和企业数据中心等。\nRoCEv1 2010年4月，IBTA发布了RoCE，此标准是作为Infiniband Architecture Specification的附加件发布的，所以也称为IBoE（InfiniBand over Ethernet）。这时的RoCE标准是在以太链路层之上用IB网络层代替了TCP/IP网络层，所以不支持IP路由功能。RoCE V1协议在以太层的typeID是0x8915。\n在RoCE中，infiniband的链路层协议头被去掉，用来表示地址的GUID被转换成以太网的MAC。Infiniband依赖于无损的物理传输，RoCE也同样依赖于无损的以太传输，这一要求会给以太网的部署带来了成本和管理上的开销。\n以太网的无损传输必须依靠L2的QoS支持，比如PFC(Priority Flow Control)，接收端在buffer池超过阈值时会向发送方发出pause帧，发送方MAC层在收到pause帧后，自动降低发送速率。这一要求，意味着整个传输环节上的所有节点包括end、switch、router，都必须全部支持L2 QoS，否则链路上的PFC就不能在两端发挥有效作用。\nRoCEv2 由于RoCEv1的数据帧不带IP头部，所以只能在L2子网内通信。为了解决此问题，IBTA于2014年提出了RoCE V2，RoCEv2扩展了RoCEv1，将GRH(Global Routing Header)换成UDP header +　IP header，扩展后的帧结构如下图所示。\n针对RoCE v1和RoCE v2，以下两点值得注意：\nRoCE v1(Layer 2)运作在Ethernet Link Layer(Layer 2)所以Ethertype 0x8915，所以正常的Frame大小为1500 bytes，而Jumbo Frame则是9000 bytes。 RoCE v2(Layer 3)运作在UDP/IPv4或UDP/IPv6之上(Layer 3)，采用UDP Port 4791进行传输。因为 RoCE v2的封包是在 Layer 3上可进行路由，所以有时又会称为Routable RoCE或简称RRoCE。 RoCEv2的普及性 生态成熟度：国内云计算与AI企业（如阿里云、腾讯云、华为云）普遍采用RoCEv2，因其兼容现有以太网架构且性价比高。例如，华为鲲鹏920芯片已支持100Gbps RoCE，并在数据中心部署中实现端到端延迟5-10μs。\n应用场景：大规模AI训练（如千卡级集群）和分布式存储（如EMC Isilon）通过RoCEv2优化通信效率，实际带宽利用率达90%以上（400G以太网实测360Gbps）。\n对比InfiniBand 维度 RoCEv2 InfiniBand 协议栈 基于UDP/IP的以太网扩展 专用协议栈（物理层到传输层独立设计） 部署成本 利用现有以太网设备，成本低 需专用交换机和网卡，成本高 扩展性 支持三层路由，扩展性强 依赖子网管理器（SM），集中式管理 性能 延迟略高（微秒级），吞吐量接近IB 超低延迟（纳秒级），吞吐量更高 适用场景 通用数据中心、云计算、AI训练 超算中心、极致性能要求的HPC环境 ","date":"2025-07-22T00:00:00Z","permalink":"https://example.com/p/roce%E4%BB%8B%E7%BB%8D/","title":"Roce介绍"},{"content":"BGP协议\n1、什么是bgp路由协议 BGP是一种基于距离矢量的路由协议，用于实现不同AS之间的路由可达。\nBGP协议的基本特点：\n（1）BGP是一种外部网关协议，其着眼点不在于发现和计算路由，而在于控制路由的传播和选择最佳路由；\n（2）BGP使用TCP作为其传输层协议（端口号179）,提高了协议的可靠性； （3）BGP是一种距离矢量路由协议，在设计上就避免了环路的发生；\n（4）BGP提供了丰富的路由策略，能够实现路由的灵活过滤和选择； （5）BGP采用触发式增量更新，而不是周期性的更新；\n2、如何建立bgp对等体 （1）TCP连接建立\n（2）BGP路由器发送OPEN报文协商参数\n（3）BGP路由器发送keepalive报文完成对等体建立\n（4）参数协商正常后双方相互发送keepalive报文，收到对方的keepalive报文后对等体建立成功，同时后续定期发送keepalive报文用于保持连接。\n（5）BGP对等体关系建立好了，就可以通过BGP update 报文通告路由到对等体。收到对方的keepalive报文后对等体建立成功，同时后续定期发送keepalive报文用于保持连接。 3、bgp的报文类型 4、bgp状态变化 （1）Idle状态是BGP初始状态。\n在Idle状态下，BGP拒绝对等体发送的连接请求。只有在收到本设备的Start事件后，BGP才开始尝试和其它BGP对等体进行TCP连接，并转至Connect状态。\nStart事件是由一个操作者配置一个BGP过程，或者重置一个已经存在的过程或者路由器软件重置BGP过程引起的。\n任何状态中收到Notification报文或TCP拆链通知等Error事件后，BGP都会转至Idle状态。\n（2）Connect状态\n在Connect状态下，BGP启动连接重传定时器（Connect Retry），等待TCP完成连接。\n如果TCP连接成功，那么BGP向对等体发送Open报文，并转至OpenSent状态。 如果TCP连接失败，那么BGP转至Active状态。 如果连接重传定时器超时，BGP仍没有收到BGP对等体的响应，那么BGP继续尝试和其它BGP对等体进行TCP连接，停留在Connect状态。 （3）Active状态\n在Active状态下，BGP总是在试图建立TCP连接。\n如果TCP连接成功，那么BGP向对等体发送Open报文，关闭连接重传定时器，并转至OpenSent状态。 如果TCP连接失败，那么BGP停留在Active状态。 如果连接重传定时器超时，BGP仍没有收到BGP对等体的响应，那么BGP转至Connect状态。 （4）Opensent状态、openconfirm状态\nTCP三次握手建立成功后，发送open报文建立对等体关系，此时的状态为 opensent状态，当收到对端回应的open报文，并且参数检查无误，在发送keepalive报文后进入openconfirm状态。\n（5）established状态\n进入openconfirm状态后，收到对端的keepalive报文后进入established状态。\n5、bgp的路由生成 BGP路由是通过BGP命令通告而成的，而通告BGP路由的方法有两种：network和Import。 （1）network方式：\n使用network命令可以将当前设备路由表中的路由（非BGP）发布到BGP路由表中并通告给邻居，和OSPF中使用network命令的方式大同小异，只不过在BGP宣告时，只需要宣告网段+掩码数即可，如：network 12.12.0.0 16。\n（2）Import方式：\n使用Import命令可以将该路由器学到的路由信息重分发到BGP路由表中，是BGP宣告路由的一种方式，可以引入BGP的路由包括：直连路由、静态路由及动态路由协议学到的路由。其命令格式与在RIP中重分发OSPF差不多。\n6、BGP通告原则 BGP设备会将最优路由加入BGP路由表，形成BGP路由。\nBGP设备与对等体建立邻居关系后，采用以下交互原则： 从IBGP对等体获得的BGP路由，BGP设备只传递给它的EBGP对等体。 从EBGP对等体获得的BGP路由，BGP设备传递给它所有EBGP和IBGP对等体（对等体是IBGP只能传递一跳，对等体是EBGP则不限制） 当存在多条到达同一目的地址的有效路由时，BGP设备只将最优路由发布给对等体 路由更新时，BGP设备只发送更新的BGP路由 所有对等体发送的路由，BGP设备都会接收 所有EBGP对等体在传递过程中下一跳改变 所有IBGP对等体在传递过程中下一跳不变（需要特别注意） 默认EBGP传递时 TTL值为1（需要特别注意） 默认IBGP传递时 TTL值为255 ","date":"2025-07-16T14:05:05+08:00","permalink":"https://example.com/p/bgp%E5%8D%8F%E8%AE%AE%E4%BB%8B%E7%BB%8D/","title":"BGP协议介绍"},{"content":"1.客户端请求 HTTPS 网址，然后连接到 server 的 443 端口 (HTTPS 默认端口，类似于 HTTP 的80端口)。\n2.采用 HTTPS 协议的服务器必须要有一套数字 CA (Certification Authority)证书，证书是需要申请的，并由专门的数字证书认证机构(CA)通过非常严格的审核之后颁发的电子证书 (当然了是要钱的，安全级别越高价格越贵)。颁发证书的同时会产生一个私钥和公钥。私钥由服务端自己保存，不可泄漏。公钥则是附带在证书的信息中，可以公开的。证书本身也附带一个证书电子签名，这个签名用来验证证书的完整性和真实性，可以防止证书被篡改。\n3.服务器响应客户端请求，将证书传递给客户端，证书包含公钥和大量其他信息，比如证书颁发机构信息，公司信息和证书有效期等。Chrome 浏览器点击地址栏的锁标志再点击证书就可以看到证书详细信息。\n4.客户端解析证书并对其进行验证。如果证书不是可信机构颁布，或者证书中的域名与实际域名不一致，或者证书已经过期，就会向访问者显示一个警告，由其选择是否还要继续通信。就像下面这样：\n如果证书没有问题，客户端就会从服务器证书中取出服务器的公钥A。然后客户端还会生成一个随机码 KEY，并使用公钥A将其加密。\n5.客户端把加密后的随机码 KEY 发送给服务器，作为后面对称加密的密钥。\n6.服务器在收到随机码 KEY 之后会使用私钥B将其解密。经过以上这些步骤，客户端和服务器终于建立了安全连接，完美解决了对称加密的密钥泄露问题，接下来就可以用对称加密愉快地进行通信了。\n7.服务器使用密钥 (随机码 KEY)对数据进行对称加密并发送给客户端，客户端使用相同的密钥 (随机码 KEY)解密数据。\n8.双方使用对称加密愉快地传输所有数据。 ","date":"2025-07-13T14:27:57+08:00","permalink":"https://example.com/p/https%E7%9A%84%E5%8E%9F%E7%90%86/","title":"Https的原理"},{"content":"Felix：运行在每一台 Host 的 agent 进程，主要负责网络接口管理和监听、路由、ARP 管理、ACL 管理和同步、状态上报等。\netcd：分布式键值存储，主要负责网络元数据一致性，确保 Calico 网络状态的准确性，可以与 kubernetes 共用；\nBGP Client（BIRD）：Calico 为每一台 Host 部署一个 BGP Client，使用 BIRD 实现，BIRD 是一个单独的持续发展的项目，实现了众多动态路由协议比如 BGP、OSPF、RIP 等。在 Calico 的角色是监听 Host 上由 Felix 注入的路由信息，然后通过 BGP 协议广播告诉剩余 Host 节点，从而实现网络互通。\nBGP Route Reflector：在大型网络规模中，如果仅仅使用 BGP client 形成 mesh 全网互联的方案就会导致规模限制，因为所有节点之间俩俩互联，需要 N^2 个连接，为了解决这个规模问题，可以采用 BGP 的 Router Reflector 的方法，使所有 BGP Client 仅与特定 RR 节点互联并做路由同步，从而大大减少连接数。\nCalico API Server 可以使用 kubectl 直接管理 Calico。\nFelix Felix 以 agent 代理的形式在每台机器端点上运行。对路由和 ACL 以及主机编程，为该主机上的端点提供所需的连接。\n根据具体的编排器环境，Felix 负责：\n接口管理\n将有关接口的信息编入内核，以便内核能够正确处理来自该端点的流量。特别是，确保主机响应来自每个工作负载的 ARP 请求，提供主机的 MAC，并为它所管理的接口启用 IP 转发。它还监控接口，以确保编程在适当的时候应用。\n路由编程\n将其主机上的端点的路由编程到 Linux 内核的 FIB（转发信息库）。这可以确保到达主机上的以这些端点为目的地的数据包被相应地转发。\nACL 编程\n在 Linux 内核中编程 ACL，以确保只有有效的流量可以在端点之间发送，并且端点不能规避 Calico 的安全措施。\n状态报告\n提供网络健康数据。特别是在配置其主机时报告错误和问题。这些数据被写入数据存储，以便对网络的其他组件和运营商可见。\nBIRD BGP Internet Routing Daemon，简称 BIRD。从 Felix 获取路由，并分发到网络上的 BGP peer，用于主机间的路由。在每个 Felix 代理的节点上运行。\nBGP 客户端负责：\n路由分配\n当 Felix 将路由插入 Linux 内核的 FIB 时，BGP 客户端将它们分配给部署中的其他节点。这确保了部署中的有效流量路由。\nBGP 路由反射器的配置\nBGP 路由反射器通常是为大型部署而配置的，而不是一个标准的 BGP 客户端。BGP 路由反射器作为连接 BGP 客户端的一个中心点。(标准 BGP 要求每个 BGP 客户端在网状拓扑结构中与其他每个 BGP 客户端连接，这很难维护)。\n为了实现冗余，你可以无缝部署多个 BGP 路由反射器。BGP 路由反射器只参与网络的控制：没有终端数据通过它们。当 Calico BGP 客户端将其 FIB 中的路由通告给路由反射器时，路由反射器将这些路由通告给部署中的其他节点。\nconfd 开源的、轻量级的配置管理工具。监控 Calico 数据存储对 BGP 配置和全局默认的日志变更，如 AS 号、日志级别和 IPAM 信息。\nConfd 根据存储中的数据更新，动态生成 BIRD 配置文件。当配置文件发生变化时，confd 会触发 BIRD 加载新的文件。\n","date":"2025-07-12T14:46:36+08:00","permalink":"https://example.com/p/calico%E7%BD%91%E7%BB%9C%E6%8F%92%E4%BB%B6%E4%BB%8B%E7%BB%8D/","title":"Calico网络插件介绍"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code A B C D E F Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus ultricies, sapien non euismod aliquam, dui ligula tincidunt odio, at accumsan nulla sapien eget ex. Proin eleifend dictum ipsum, non euismod ipsum pulvinar et. Vivamus sollicitudin, quam in pulvinar aliquam, metus elit pretium purus Proin sit amet velit nec enim imperdiet vehicula. Ut bibendum vestibulum quam, eu egestas turpis gravida nec Sed scelerisque nec turpis vel viverra. Vivamus vitae pretium sapien Code Blocks Code block with backticks 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode 1 2 3 4 5 6 7 8 9 10 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Diff code block 1 2 3 4 5 [dependencies.bevy] git = \u0026#34;https://github.com/bevyengine/bevy\u0026#34; rev = \u0026#34;11f52b8c72fc3a568e8bb4a4cd1f3eb025ac2e13\u0026#34; - features = [\u0026#34;dynamic\u0026#34;] + features = [\u0026#34;jpeg\u0026#34;, \u0026#34;dynamic\u0026#34;] List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL + ALT + Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nHyperlinked image The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://example.com/p/markdown-syntax-guide/pawel-czerwinski-8uZPynIu-rQ-unsplash_hu_e95a4276bf860a84.jpg","permalink":"https://example.com/p/markdown-syntax-guide/","title":"Markdown Syntax Guide"}]